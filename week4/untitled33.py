# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5knQf2JJqjDXrwINlaOQW0odhHXpGMm
"""

import torch
import torch.nn as nn
import torch.optim as optim



corpus = """
artificial intelligence is transforming modern society
machine learning allows systems to improve automatically with experience
data plays a critical role in training intelligent systems
recurrent neural networks handle sequential data
lstm and gru models address long term dependency problems
"""

words = corpus.lower().split()

vocab = sorted(set(words))
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for w, i in word_to_idx.items()}

vocab_size = len(vocab)

data = [word_to_idx[w] for w in words]
data = torch.tensor(data)



X = data[:-1]
y = data[1:]

X = X.unsqueeze(0)  # batch dimension
y = y.unsqueeze(0)



class GRUModel(nn.Module):
    def __init__(self, vocab_size, embed_size=64, hidden_size=128, num_layers=1):
        super(GRUModel, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)

        self.gru = nn.GRU(
            embed_size,
            hidden_size,
            num_layers=num_layers,
            batch_first=True
        )

        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        x = self.embedding(x)
        out, hidden = self.gru(x, hidden)
        out = self.fc(out)
        return out, hidden



model = GRUModel(vocab_size)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 200



for epoch in range(epochs):
    optimizer.zero_grad()

    output, _ = model(X)

    loss = criterion(
        output.view(-1, vocab_size),
        y.view(-1)
    )

    loss.backward()
    optimizer.step()

    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")



def generate_text(seed_text, num_words=15):
    model.eval()

    words_list = seed_text.lower().split()
    hidden = None

    for _ in range(num_words):
        input_seq = torch.tensor(
            [[word_to_idx[w] for w in words_list]]
        )

        with torch.no_grad():
            output, hidden = model(input_seq, hidden)

        next_word_logits = output[0, -1]
        predicted_idx = torch.argmax(next_word_logits).item()

        predicted_word = idx_to_word[predicted_idx]
        words_list.append(predicted_word)

    return " ".join(words_list)

print("\nGenerated Text:\n")
print(generate_text("artificial intelligence", 15))