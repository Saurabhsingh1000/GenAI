# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5knQf2JJqjDXrwINlaOQW0odhHXpGMm
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

corpus = """
artificial intelligence is transforming modern society.
it is used in healthcare finance education and transportation.
machine learning allows systems to improve automatically with experience.
data plays a critical role in training intelligent systems.
large datasets help models learn complex patterns.
deep learning uses multi layer neural networks.
neural networks are inspired by biological neurons.
each neuron processes input and produces an output.
training a neural network requires optimization techniques.
gradient descent minimizes the loss function.
"""

# Clean text
corpus = corpus.lower().replace(".", "")
words = corpus.split()

# Vocabulary
vocab = sorted(set(words))
word_to_idx = {word: i for i, word in enumerate(vocab)}
idx_to_word = {i: word for word, i in word_to_idx.items()}

vocab_size = len(vocab)

sequence_length = 3
X = []
y = []

for i in range(len(words) - sequence_length):
    seq = words[i:i+sequence_length]
    target = words[i+sequence_length]

    X.append([word_to_idx[word] for word in seq])
    y.append(word_to_idx[target])

X = torch.tensor(X)
y = torch.tensor(y)


class RNNModel(nn.Module):
    def __init__(self, vocab_size, embed_size=32, hidden_size=64):
        super(RNNModel, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])  # Use last output
        return out
model = RNNModel(vocab_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)


epochs = 300

for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 50 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")


def generate_text(seed_text, num_words=20):
    model.eval()

    words_list = seed_text.lower().split()

    for _ in range(num_words):
        seq = words_list[-sequence_length:]
        seq_idx = torch.tensor([[word_to_idx[word] for word in seq]])

        with torch.no_grad():
            output = model(seq_idx)
            predicted_idx = torch.argmax(output).item()

        predicted_word = idx_to_word[predicted_idx]
        words_list.append(predicted_word)

    return " ".join(words_list)
print("\n--- RNN Generated Text ---\n")
print(generate_text("artificial intelligence is", 20))