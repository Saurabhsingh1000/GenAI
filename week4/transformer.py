# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5knQf2JJqjDXrwINlaOQW0odhHXpGMm
"""

import torch
import torch.nn as nn
import torch.optim as optim
import math



corpus = """
artificial intelligence is transforming modern society
machine learning allows systems to improve automatically with experience
data plays a critical role in training intelligent systems
transformer models changed the field of nlp
attention allows the model to focus on relevant context
modern language models are based on transformers
"""

words = corpus.lower().split()

vocab = sorted(set(words))
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for w, i in word_to_idx.items()}

vocab_size = len(vocab)

data = [word_to_idx[w] for w in words]
data = torch.tensor(data)



X = data[:-1]
y = data[1:]

X = X.unsqueeze(0)  # batch dimension
y = y.unsqueeze(0)



class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=500):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x



class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=256,
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoder(x)

        mask = nn.Transformer.generate_square_subsequent_mask(x.size(1)).to(x.device)

        x = self.transformer(x, mask)
        out = self.fc(x)
        return out



model = TransformerModel(vocab_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

epochs = 200



for epoch in range(epochs):
    optimizer.zero_grad()

    output = model(X)

    loss = criterion(
        output.view(-1, vocab_size),
        y.view(-1)
    )

    loss.backward()
    optimizer.step()

    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")



def generate_text(seed_text, num_words=15):
    model.eval()

    words_list = seed_text.lower().split()

    for _ in range(num_words):
        input_seq = torch.tensor(
            [[word_to_idx[w] for w in words_list]]
        )

        with torch.no_grad():
            output = model(input_seq)

        next_word_logits = output[0, -1]
        predicted_idx = torch.argmax(next_word_logits).item()

        predicted_word = idx_to_word[predicted_idx]
        words_list.append(predicted_word)

    return " ".join(words_list)

print("\nGenerated Text:\n")
print(generate_text("artificial intelligence", 15))